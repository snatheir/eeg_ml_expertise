{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing Artificial Intelligence and Electroencephalography to Assess Expertise on a Simulated Neurosurgical Task\n",
    "## Sample data available  [HERE](https://www.dropbox.com/s/s86ntxdfayizwcl/data-sample.csv?dl=0)\n",
    "### Submitted to the Journal of Surgical Education 2021-08-27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAL\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import scipy\n",
    "import itertools\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pylab import rcParams\n",
    "from time import time\n",
    "from joblib import dump, load\n",
    "\n",
    "# TENSORFLOW\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# SKLEARN\n",
    "import sklearn\n",
    "from sklearn import neighbors\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# CLASSICAL ML MODELS\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "binary_labels = ['Neurosurgeon/Sr. Resident', 'Jr. Resident/Medical Student']\n",
    "\n",
    "# MODEL INTERPRETATION\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ DATA\n",
    "data = pd.read_csv('data.csv', index_col='Subject')\n",
    "data = data.drop([\"Stage\"], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETERMINING TESTING SET\n",
    "\n",
    "skilled = data[data['Group']==0]\n",
    "less_skilled = data[data['Group']==1]\n",
    "\n",
    "skilled_nums = skilled.index.unique()\n",
    "less_skilled_nums = less_skilled.index.unique()\n",
    "\n",
    "skilled_sample = np.random.choice(skilled_nums, size=2, replace=False)\n",
    "less_skilled_sample = np.random.choice(less_skilled_nums, size=3, replace=False)\n",
    "\n",
    "testing_set = np.concatenate((skilled_sample, less_skilled_sample))\n",
    "testing_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRICS - area under the receiver operating curve\n",
    "auc = tf.keras.metrics.AUC(\n",
    "    num_thresholds=200, curve='ROC', summation_method='interpolation', name=None,\n",
    "    dtype=None, thresholds=None, multi_label=False, label_weights=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALLBACKS\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=30, mode='min', verbose=1)\n",
    "checkpoint = ModelCheckpoint('model_best_weights.h5', monitor='val_auc', verbose=1, save_best_only=True, mode='min', save_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHHOLD SOME TESTING DATA\n",
    "testing_data = data[data.index.isin(testing_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data =  data[~data.index.isin(testing_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_z.keys())]))\n",
    "    model.add(tf.keras.layers.Dropout(0.8))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                optimizer=\"adam\",\n",
    "                metrics=[\"acc\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(model, x_train, x_test, y_train, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    y_val_cat_prob=model.predict(x_test)\n",
    "    acc = accuracy_score(y_test, y_val_cat_prob)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "LABEL = \"Group\"\n",
    "\n",
    "# SAVE DATA AND MODEL FOR EACH OF THE LOOCV ITERATIONS\n",
    "models = []\n",
    "scores = []\n",
    "history = []\n",
    "test_predictions = []\n",
    "y_trains = []\n",
    "y_vals = []\n",
    "x_trains = []\n",
    "x_vals = []\n",
    "\n",
    "# SAVE FINAL SCORES\n",
    "scores_rf = []\n",
    "scores_logistic = []\n",
    "scores_svm = []\n",
    "scores_nb = []\n",
    "scores_lda = []\n",
    "scores_knn = []\n",
    "\n",
    "\n",
    "subjects = training_data.index.unique()\n",
    "\n",
    "# LEAVE-ONE-OUT-CROSS-VALIDATION (LOOCV)\n",
    "for i in range(len(subjects)):\n",
    "    # CHOOSE VALIDATION SUBJECT\n",
    "    val_subject = subjects[i]\n",
    "    train_subjects = [subject for subject in subjects if subject != val_subject]\n",
    "    print(\"Testing on subject #\", val_subject)\n",
    "    \n",
    "    # MAKE TRAINING AND VALIDATION DATAFRAMES\n",
    "    train_df = training_data[training_data.index.isin(train_subjects)]\n",
    "    val_df = training_data[training_data.index == val_subject]\n",
    "    \n",
    "    # KEEP TRACK OF Y AND REMOVE IT FROM TRAINING/VALIDATION DATA\n",
    "    y_trains.append(np.array(train_df[LABEL]))\n",
    "    y_vals.append(np.array(val_df[LABEL]))\n",
    "    train_df = train_df.drop([LABEL],axis=1)\n",
    "    val_df = val_df.drop([LABEL],axis=1)\n",
    "    \n",
    "    # NORMALIZE\n",
    "    train_z = train_df.copy(deep=True)\n",
    "    val_z = val_df.copy(deep=True)\n",
    "    cols = list(train_df.columns)\n",
    "    for col in cols:\n",
    "        train_z[col] = (train_df[col] - train_df[col].mean())/train_df[col].std(ddof=0)\n",
    "        val_z[col] = (val_df[col] - train_df[col].mean())/train_df[col].std(ddof=0)\n",
    "    \n",
    "    # EXPORT NORMALIZED TRAINING AND VALIDATION DATA\n",
    "    train_z.to_csv(str(i).zfill(2)+\"_train_\"+str(val_subject).zfill(2)+\".csv\")\n",
    "    val_z.to_csv(str(i).zfill(2)+\"_test_\"+str(val_subject).zfill(2)+\".csv\")\n",
    "    \n",
    "    # KEEP TRACK OF TRAINING AND VALIDATION NORMALIZED DATA\n",
    "    x_trains.append(train_z)\n",
    "    x_vals.append(val_z)\n",
    "        \n",
    "    # BUILD MODEL - TUNE THIS WITH EACH HYPERPARAMETER TUNING EXPERIMENT\n",
    "    models.append(build_model())\n",
    "    rf = RandomForestClassifier(n_estimators = 45, max_depth=3, random_state=0)\n",
    "    machine = svm.SVC(gamma='auto', kernel=\"rbf\", decision_function_shape=\"ovo\")\n",
    "    logistic = LogisticRegression(solver='liblinear',multi_class='ovr')\n",
    "    nb = GaussianNB()\n",
    "    knn = KNeighborsClassifier()\n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    \n",
    "    # EVALUATE MODELS\n",
    "    history.append(models[i].fit(x_trains[i], y_trains[i],epochs=EPOCHS,callbacks = [early_stop], validation_data=(x_vals[i], y_vals[i])))\n",
    "    scores.append(models[i].evaluate(x_vals[i],y_vals[i]))\n",
    "    test_predictions.append(models[i].predict(x_vals[i]).flatten())\n",
    "    scores_rf.append(get_score(rf, x_trains[i], x_vals[i], y_trains[i], y_vals[i]))\n",
    "    scores_logistic.append(get_score(logistic, x_trains[i], x_vals[i], y_trains[i], y_vals[i]))  \n",
    "    scores_svm.append(get_score(machine, x_trains[i], x_vals[i], y_trains[i], y_vals[i]))\n",
    "    scores_nb.append(get_score(nb, x_trains[i], x_vals[i], y_trains[i], y_vals[i]))\n",
    "    scores_lda.append(get_score(lda, x_trains[i], x_vals[i], y_trains[i], y_vals[i]))\n",
    "    scores_knn.append(get_score(knn, x_trains[i], x_vals[i], y_trains[i], y_vals[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET AVERAGE MODEL SCORES\n",
    "subject_scores = [score[1] for score in scores]\n",
    "print(\"ANN:\", statistics.mean(subject_scores))\n",
    "print(\"RF:\", statistics.mean(scores_rf))\n",
    "print(\"SVM:\", statistics.mean(scores_svm))\n",
    "print(\"LR:\", statistics.mean(scores_logistic))\n",
    "print(\"NB:\", statistics.mean(scores_nb))\n",
    "print(\"KNN:\", statistics.mean(scores_knn))\n",
    "print(\"LDA:\", statistics.mean(scores_lda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDENTIFY RESECTION MISCLASSIFICATIONS BY SUBJECT\n",
    "subject_scores_dict = {}\n",
    "i=0\n",
    "for subject in subjects:\n",
    "    subject_scores_dict[subject] = [subject_scores[i], scores_rf[i], scores_svm[i],scores_logistic[i],scores_nb[i],scores_knn[i],scores_lda[i],]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE LOOCV RESULTS\n",
    "preds = pd.pandas.DataFrame.from_dict(subject_scores_dict, orient=\"index\", columns=[\"ANN\", \"RF\", \"SVM\", \"LR\", \"NB\", \"KNN\", \"LDA\"])\n",
    "preds.to_csv('loocv.csv')\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN FINAL MODEL OF EACH TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(model, x_train, x_test, y_train, y_test):\n",
    "    model.fit(x_train, y_train)\n",
    "    y_val_cat_prob=model.predict(x_test)\n",
    "    fpr , tpr , thresholds = roc_curve(y_test, y_val_cat_prob)\n",
    "    auroc = roc_auc_score(y_test, y_val_cat_prob)\n",
    "    acc = accuracy_score(y_test, y_val_cat_prob)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_val_cat_prob).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    \n",
    "    f = (2*precision*recall) / (precision + recall)\n",
    "\n",
    "    return sensitivity, specificity, acc, auroc, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "\n",
    "subjects = data.index.unique()\n",
    "\n",
    "# DROP LABEL\n",
    "y_train = np.array(training_data[LABEL])\n",
    "y_test = np.array(testing_data[LABEL])\n",
    "train_df = training_data.drop([LABEL],axis=1)\n",
    "test_df = testing_data.drop([LABEL],axis=1)\n",
    "test_df.to_csv(str(i)+\"_TEST_FINAL\"+\".csv\")\n",
    "train_df.to_csv(str(i)+\"_TRAIN_FINAL\"+\".csv\")\n",
    "\n",
    "# NORMALIZE\n",
    "x_train = train_df.copy(deep=True)\n",
    "x_test = test_df.copy(deep=True)\n",
    "columns = train_df.columns\n",
    "cols = list([col for col in columns if \"Stage\" not in col])\n",
    "for col in cols:\n",
    "    x_train[col] = (train_df[col] - train_df[col].mean())/train_df[col].std(ddof=0)\n",
    "    x_test[col] = (test_df[col] - train_df[col].mean())/train_df[col].std(ddof=0)\n",
    "\n",
    "# SAVE NORMALIZED DATA\n",
    "x_test.to_csv(str(i)+\"_TEST_FINAL_NORMALIZED\"+\".csv\")\n",
    "x_train.to_csv(str(i)+\"_TRAIN_FINAL_NORMALIZED\"+\".csv\")    \n",
    "\n",
    "# BUILD MODELS WITH MOST SUCCESSFUL HYPERPARAMETERS HERE\n",
    "ann = build_model()\n",
    "rf = RandomForestClassifier(n_estimators = 45, max_depth=3, random_state=0)\n",
    "machine = svm.SVC(gamma='auto', kernel=\"rbf\", decision_function_shape=\"ovo\")\n",
    "logistic = LogisticRegression(solver='liblinear',multi_class='ovr')\n",
    "nb = GaussianNB()\n",
    "knn = KNeighborsClassifier()\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# EVALUATE\n",
    "history = ann.fit(x_train, y_train,epochs=EPOCHS,callbacks = [early_stop], validation_data=(x_test, y_test))\n",
    "score_ann = ann.evaluate(x_test,y_test)\n",
    "score_rf = get_score(rf, x_train, x_test, y_train, y_test)\n",
    "score_logistic = get_score(logistic, x_train, x_test, y_train, y_test)\n",
    "score_svm = get_score(machine, x_train, x_test, y_train, y_test)\n",
    "score_nb = get_score(nb, x_train, x_test, y_train, y_test)\n",
    "score_lda = get_score(lda, x_train, x_test, y_train, y_test)\n",
    "score_knn = get_score(knn, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE FULL METRICS FOR ANN\n",
    "y_val_cat_prob=ann.predict(x_test)\n",
    "auroc = roc_auc_score(y_test, y_val_cat_prob)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_val_cat_prob.round()).ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f = (2*precision*recall) / (precision + recall)\n",
    "score_ann = [sensitivity, specificity, score_ann[1], auroc, f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT FINAL RESULTS\n",
    "results = pd.DataFrame({\"ANN\":score_ann,\"RF\":score_rf, \"SVM\":score_svm, \"LR\":score_logistic, \"NB\":score_nb, \"KNN\":score_knn, \"LDA\":score_lda})\n",
    "results.index=[\"Sensitivity\", \"Specificity\", \"Accuracy\", \"AUROC\", \"F-Score\"]\n",
    "results = results.T\n",
    "results = results.sort_values('F-Score', ascending=False)\n",
    "results.to_csv(\"results.csv\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODELS\n",
    "ann.save('model_ann.h5')\n",
    "dump(rf, \"model_rf.joblib\")\n",
    "dump(machine, \"model_machine.joblib\")\n",
    "dump(logistic, \"model_logistic.joblib\")\n",
    "dump(nb, \"model_nb.joblib\")\n",
    "dump(knn, \"model_knn.joblib\")\n",
    "dump(lda, \"model_lda.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL INTERPRETABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.concat([x_train,x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Kernel SHAP to explain test set predictions\n",
    "model = ann\n",
    "explainer = shap.KernelExplainer(model.predict, x)\n",
    "shap_values = explainer.shap_values(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, x.columns, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
